# -*- coding: utf-8 -*-
"""Boston House Price Prediction Neural Networks.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tdTRLgVb52pVBILz2E-RuJeoPDPpE3iw

# **Import Libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as snsw

housing = pd.read_csv('Boston House Price Data.csv')
housing.head()

housing.isnull().sum()

"""No Missing Vals, hence skipping EDA and going ahead with train-test split

# **Train-Test Split**
"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X = housing.drop('PRICE', axis=1)
y = housing['PRICE']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""# **Scaling Data**
*Without scaling neural networks perform bad.*
"""

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""# **Build a Neural Network**"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential([
    Dense(32, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(16, activation='relu'),
    Dense(1)
])

model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])

"""# **Train The Model**"""

history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)

"""# **Model Evaluation**"""

test_loss, test_mae = model.evaluate(X_test, y_test)
final_predictions = model.predict(X_test)
print(f'Test Loss: {test_loss:.4f}')
print(f'Test MAE: {test_mae}')

"""# **Save The Model**"""

model.save('housing_price_nn.h5')

"""# **Plot the Losses**"""

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""# **Predictions v/s. Actual Prices**"""

plt.scatter(y_test, final_predictions)
plt.xlabel("Actual Prices")
plt.ylabel("Predicted Prices")
plt.title("Actual vs Predicted")
plt.show()

"""**Summary / Why I Did What**
- Used train/validation/test split to fairly evaluate model performance and avoid data leakage.
- Built a pipeline with SimpleImputer and StandardScaler to handle missing values and scale features (essential for neural networks).
- Chose a small neural network (32 → 16 → 1) because the dataset is small (506 samples) → prevents overfitting.
- Used mean squared error (MSE) as loss and mean absolute error (MAE) as metric for regression evaluation.
- Trained for 100 epochs and monitored validation metrics to ensure the network is learning and not overfitting.
- Observed that validation MAE closely matches test MAE → model generalizes reasonably well.
- Neural nets offer practice with deep learning, even if tree-based models often perform better on small tabular datasets.
"""